{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp64 import data\n",
    "from mlp64 import experiment\n",
    "from mlp64 import models\n",
    "from mlp64 import st\n",
    "from mlp64 import resnet2d\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "import librosa\n",
    "import scipy.fft as scipyfft\n",
    "librosa.set_fftlib(scipyfft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
    "    'vgg19_bn', 'vgg19',\n",
    "]\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
    "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
    "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
    "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
    "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
    "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, features, num_classes=1000, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 1\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfgs = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg11(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 11-layer model (configuration \"A\") from\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg11', 'A', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg11_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg11_bn', 'A', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 13-layer model (configuration \"B\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg13', 'B', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg13_bn', 'B', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 16-layer model (configuration \"D\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg16', 'D', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg16_bn', 'D', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 19-layer model (configuration \"E\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg19', 'E', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg19_bn', 'E', True, pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = lambda y: torch.as_tensor(np.abs(librosa.cqt(y.numpy().ravel(), sr=16000, hop_length=192))[np.newaxis, :])\n",
    "#transform = ta.transforms.MelSpectrogram(n_fft=800, hop_length=160)\n",
    "n_fft = 512\n",
    "hop_length = 128\n",
    "window = torch.hann_window(n_fft)\n",
    "#transform = lambda y: torch.as_tensor(st.stft(y.numpy(), n_fft, hop_length))[None, :]\n",
    "\n",
    "transform = lambda x: st.stft_torch(x, n_fft, hop_length, window)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON loaded into DataFrame!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Desktop/MLP64/mlp64/data.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"instrument_class\"] = df.apply(lambda x: x[\"instrument_source\"] * 11 + x[\"instrument_family\"], axis=1)\n"
     ]
    }
   ],
   "source": [
    "path = Path(\"/home/tim/Desktop/MLP64/dataset/nsynth-test/\")\n",
    "vdf = data.create_dataset_df(path / \"examples.json\")\n",
    "target = \"instrument_class\"\n",
    "trdf, tedf = data.get_train_test(vdf, target)\n",
    "trds = data.CachedNSynth(path / \"audio\", trdf, target_field=target, transform=transform, cache=\"cache2\", overwrite=True)\n",
    "teds = data.CachedNSynth(path / \"audio\", tedf, target_field=target, transform=transform, cache=\"cache2\", overwrite=True)\n",
    "\n",
    "batch_size = 32\n",
    "trloader = DataLoader(trds, batch_size=batch_size, num_workers=6, shuffle=True)\n",
    "teloader = DataLoader(teds, batch_size=batch_size, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON loaded into DataFrame!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Desktop/MLP64/mlp64/data.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"instrument_class\"] = df.apply(lambda x: x[\"instrument_source\"] * 11 + x[\"instrument_family\"], axis=1)\n"
     ]
    }
   ],
   "source": [
    "path = Path(\"/home/tim/Desktop/MLP64/dataset/nsynth-valid/\")\n",
    "vdf = data.create_dataset_df(path / \"examples.json\")\n",
    "target = \"instrument_class\"\n",
    "trdf, tedf = data.get_train_test(vdf, target)\n",
    "trds2 = data.CachedNSynth(path / \"audio\", trdf, target_field=target, transform=transform, cache=\"cache\", overwrite=True)\n",
    "teds2 = data.CachedNSynth(path / \"audio\", tedf, target_field=target, transform=transform, cache=\"cache\", overwrite=True)\n",
    "\n",
    "batch_size = 128\n",
    "trloader2 = DataLoader(trds2, batch_size=batch_size, num_workers=6, shuffle=False)\n",
    "teloader2 = DataLoader(teds2, batch_size=batch_size, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/home/tim/Desktop/MLP64/dataset/nsynth-train/\")\n",
    "vdf = data.create_dataset_df(path / \"examples.json\")\n",
    "target = \"instrument_family\"\n",
    "trdf, tedf = data.get_train_test(vdf, target)\n",
    "trds3 = data.NSynth(path / \"audio\", trdf, target_field=target, transform=transform)\n",
    "teds3 = data.NSynth(path / \"audio\", tedf, target_field=target, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "trloader3 = DataLoader(trds3, batch_size=batch_size, num_workers=6, shuffle=True)\n",
    "teloader3 = DataLoader(teds3, batch_size=batch_size, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fft_bins is this for n_fft=512, hop_length=128\n",
    "#model = models.resnet18(num_classes=33, n_fft_bins=257)\n",
    "model = resnet2d.ResNet(resnet2d.BasicBlock, [1, 1, 1, 1], num_classes=33, norm_layer=nn.InstanceNorm2d)\n",
    "#model = vgg11(pretrained=False, num_classes=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.4101, accuracy: 0.4444: 100%|██████████| 80/80 [00:22<00:00,  3.48it/s]\n",
      "loss: 1.1452, accuracy: 0.7586: 100%|██████████| 20/20 [00:06<00:00,  2.92it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_acc_0.3993_train_loss_1.9494_val_acc_0.6364_val_loss_1.3235 epoch time 29.8503 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.8215, accuracy: 0.7778: 100%|██████████| 80/80 [00:10<00:00,  7.36it/s]\n",
      "loss: 0.5563, accuracy: 0.7586: 100%|██████████| 20/20 [00:01<00:00, 17.40it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_acc_0.6750_train_loss_1.0344_val_acc_0.7457_val_loss_0.7734 epoch time 12.0177 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2598, accuracy: 0.9444: 100%|██████████| 80/80 [00:10<00:00,  7.30it/s]\n",
      "loss: 0.2239, accuracy: 1.0000: 100%|██████████| 20/20 [00:01<00:00, 17.20it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_acc_0.8470_train_loss_0.5859_val_acc_0.9344_val_loss_0.3726 epoch time 12.1204 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2392, accuracy: 0.9444: 100%|██████████| 80/80 [00:11<00:00,  7.25it/s]\n",
      "loss: 0.1588, accuracy: 1.0000: 100%|██████████| 20/20 [00:01<00:00, 16.77it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_acc_0.9302_train_loss_0.2930_val_acc_0.9375_val_loss_0.2682 epoch time 12.2289 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0642, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.25it/s]\n",
      "loss: 0.0904, accuracy: 0.9655: 100%|██████████| 20/20 [00:01<00:00, 17.11it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_acc_0.9590_train_loss_0.1766_val_acc_0.9608_val_loss_0.1528 epoch time 12.2047 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0499, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.21it/s]\n",
      "loss: 0.0518, accuracy: 1.0000: 100%|██████████| 20/20 [00:01<00:00, 17.45it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_acc_0.9703_train_loss_0.1277_val_acc_0.9766_val_loss_0.1013 epoch time 12.2501 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0408, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.23it/s]\n",
      "loss: 0.0643, accuracy: 0.9655: 100%|██████████| 20/20 [00:01<00:00, 16.79it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_acc_0.9867_train_loss_0.0629_val_acc_0.9639_val_loss_0.0988 epoch time 12.2608 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0730, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.25it/s]\n",
      "loss: 0.1381, accuracy: 0.9655: 100%|██████████| 20/20 [00:01<00:00, 17.43it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_acc_0.9891_train_loss_0.0544_val_acc_0.9373_val_loss_0.1693 epoch time 12.1820 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0133, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.22it/s]\n",
      "loss: 0.0154, accuracy: 1.0000: 100%|██████████| 20/20 [00:01<00:00, 17.14it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train_acc_0.9941_train_loss_0.0406_val_acc_0.9922_val_loss_0.0420 epoch time 12.2442 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0128, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.24it/s]\n",
      "loss: 0.0042, accuracy: 1.0000: 100%|██████████| 20/20 [00:01<00:00, 17.27it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train_acc_0.9988_train_loss_0.0131_val_acc_0.9969_val_loss_0.0252 epoch time 12.2185 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0032, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.22it/s]\n",
      "loss: 0.0031, accuracy: 1.0000: 100%|██████████| 20/20 [00:01<00:00, 17.04it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train_acc_1.0000_train_loss_0.0065_val_acc_0.9984_val_loss_0.0186 epoch time 12.2536 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0023, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.24it/s]\n",
      "loss: 0.0024, accuracy: 1.0000: 100%|██████████| 20/20 [00:01<00:00, 17.02it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train_acc_1.0000_train_loss_0.0040_val_acc_0.9984_val_loss_0.0176 epoch time 12.2338 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0137, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.16it/s]\n",
      "loss: 0.0021, accuracy: 1.0000: 100%|██████████| 20/20 [00:01<00:00, 17.15it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train_acc_1.0000_train_loss_0.0032_val_acc_0.9984_val_loss_0.0173 epoch time 12.3349 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0041, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.20it/s]\n",
      "loss: 0.0018, accuracy: 1.0000: 100%|██████████| 20/20 [00:01<00:00, 17.07it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train_acc_1.0000_train_loss_0.0027_val_acc_0.9984_val_loss_0.0162 epoch time 12.2821 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0035, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.13it/s]\n",
      "loss: 0.0016, accuracy: 1.0000: 100%|██████████| 20/20 [00:01<00:00, 15.89it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train_acc_1.0000_train_loss_0.0023_val_acc_0.9984_val_loss_0.0156 epoch time 12.4838 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0012, accuracy: 1.0000: 100%|██████████| 80/80 [00:11<00:00,  7.16it/s]\n",
      "loss: 0.0014, accuracy: 1.0000: 100%|██████████| 20/20 [00:01<00:00, 16.54it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train_acc_1.0000_train_loss_0.0020_val_acc_0.9984_val_loss_0.0157 epoch time 12.3895 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0025, accuracy: 1.0000:  46%|████▋     | 37/80 [00:05<00:06,  6.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e4fd64ce177a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train on NSynth test dataset (smallest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../experiments/classifier_instancenorm2d/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_from_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/MLP64/mlp64/experiment.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0mcurrent_epoch_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0mcurrent_epoch_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MLP64/mlp64/experiment.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Compute argmax of predictions to get accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train on NSynth test dataset (smallest)\n",
    "exp = experiment.Experiment(model, \"../experiments/classifier_instancenorm2d/\", 10, trloader, teloader, continue_from_epoch=-1)\n",
    "exp.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0083, accuracy: 1.0000: 100%|██████████| 63/63 [01:14<00:00,  1.18s/it]\n",
      "loss: 0.0054, accuracy: 1.0000: 100%|██████████| 16/16 [00:18<00:00,  1.16s/it]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train_acc_0.9909_train_loss_0.0350_val_acc_0.9985_val_loss_0.0093 epoch time 92.6542 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0058, accuracy: 1.0000: 100%|██████████| 63/63 [00:32<00:00,  1.95it/s]\n",
      "loss: 0.0073, accuracy: 1.0000: 100%|██████████| 16/16 [00:02<00:00,  5.52it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train_acc_0.9980_train_loss_0.0093_val_acc_0.9980_val_loss_0.0150 epoch time 35.2201 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0015, accuracy: 1.0000: 100%|██████████| 63/63 [00:32<00:00,  1.92it/s]\n",
      "loss: 0.0013, accuracy: 1.0000: 100%|██████████| 16/16 [00:03<00:00,  5.33it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: train_acc_0.9996_train_loss_0.0041_val_acc_0.9995_val_loss_0.0030 epoch time 35.8501 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0008, accuracy: 1.0000: 100%|██████████| 63/63 [00:32<00:00,  1.92it/s]\n",
      "loss: 0.0008, accuracy: 1.0000: 100%|██████████| 16/16 [00:03<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: train_acc_1.0000_train_loss_0.0013_val_acc_1.0000_val_loss_0.0021 epoch time 35.8908 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_acc': [0.9909474206349206, 0.998015873015873, 0.9996279761904762, 1.0],\n",
       " 'train_loss': [0.035000756, 0.009342753, 0.004145426, 0.001289057],\n",
       " 'val_acc': [0.99853515625, 0.998046875, 0.99951171875, 1.0],\n",
       " 'val_loss': [0.009302532, 0.0149792805, 0.002967929, 0.0021318619]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue training from epoch 10 on validation dataset (medium [1GB])\n",
    "exp2 = experiment.Experiment(model, \"../experiments/classifier_instancenorm2d/\", 20, trloader2, teloader2, continue_from_epoch=-2)\n",
    "exp2.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU 0\n"
     ]
    }
   ],
   "source": [
    "# Continue training from epoch 20 on training dataset (large [37GB])\n",
    "exp3 = experiment.Experiment(model, \"q1\", 22, trloader3, teloader3, continue_from_epoch=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2156, accuracy: 0.9216: 100%|██████████| 2802/2802 [06:27<00:00,  7.23it/s]\n",
      "loss: 1.5690, accuracy: 0.5172: 100%|██████████| 701/701 [00:38<00:00, 18.22it/s]\n",
      "  0%|          | 0/2802 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train_acc_0.8018_train_loss_0.5716_val_acc_0.5058_val_loss_1.6210 epoch time 426.1866 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0899, accuracy: 0.9804: 100%|██████████| 2802/2802 [06:30<00:00,  7.17it/s]\n",
      "loss: 0.4159, accuracy: 0.9310: 100%|██████████| 701/701 [00:38<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: train_acc_0.9392_train_loss_0.1739_val_acc_0.9279_val_loss_0.2184 epoch time 428.9713 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_acc': [0.8017876761696827, 0.9391603380288589],\n",
       " 'train_loss': [0.5715694, 0.17389172],\n",
       " 'val_acc': [0.5057530190860348, 0.927861675439028],\n",
       " 'val_loss': [1.6209569, 0.21836306]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp3.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "(From this run:)\n",
    "```\n",
    "loss: 2.1420, accuracy: 0.2549: 100%|██████████| 2802/2802 [09:13<00:00,  5.06it/s]\n",
    "loss: 2.3139, accuracy: 0.1379: 100%|██████████| 701/701 [01:27<00:00,  7.98it/s]\n",
    "  0%|          | 0/2802 [00:00<?, ?it/s]\n",
    "\n",
    "Epoch 0: train_acc_0.1815_train_loss_2.2424_val_acc_0.1846_val_loss_2.2145 epoch time 641.8255 seconds\n",
    "\n",
    "loss: 2.1385, accuracy: 0.2549: 100%|██████████| 2802/2802 [08:59<00:00,  5.20it/s]\n",
    "loss: 2.3097, accuracy: 0.1379: 100%|██████████| 701/701 [01:27<00:00,  8.04it/s]\n",
    "  0%|          | 0/2802 [00:00<?, ?it/s]\n",
    "\n",
    "Epoch 1: train_acc_0.1846_train_loss_2.2140_val_acc_0.1846_val_loss_2.2140 epoch time 626.3854 seconds\n",
    "\n",
    "loss: 2.1383, accuracy: 0.2549: 100%|██████████| 2802/2802 [09:13<00:00,  5.06it/s]\n",
    "loss: 2.3093, accuracy: 0.1379: 100%|██████████| 701/701 [01:27<00:00,  8.02it/s]\n",
    "  0%|          | 0/2802 [00:00<?, ?it/s]\n",
    "\n",
    "Epoch 2: train_acc_0.1846_train_loss_2.2139_val_acc_0.1846_val_loss_2.2139 epoch time 641.2183 seconds\n",
    "\n",
    "loss: 2.3469, accuracy: 0.1875:   5%|▍         | 127/2802 [00:25<08:52,  5.03it/s]\n",
    "```\n",
    "\n",
    "\n",
    "* While both losses decreas marginally, validation accuracy stays where it is; train accuracy also only increases marginally\n",
    "\n",
    "Todo:\n",
    "\n",
    "* Inspect confusion matrix\n",
    "\n",
    "# Remark after the fact:\n",
    "\n",
    "* The problem was caused by not normalising the input\n",
    "\n",
    "Normalising fixes the issue and makes the network train.\n",
    "\n",
    "Still, there can be big fluctuations in validation accuracy between epochs, perhaps warranting (stronger) regularisation techniques.\n",
    "\n",
    "# TODO:\n",
    "\n",
    "* Confusion matrix saving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
